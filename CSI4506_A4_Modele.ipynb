{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbcc9QS1tnBN"
      },
      "source": [
        "**DEVOIR 4 - Classification de textes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Eeke4Z_EkW"
      },
      "source": [
        "1. Numéro du groupe: 7\n",
        "   Noms des membres: Karim Benhallam\n",
        "   Numéros d'étudiants des membres: 300100867\n",
        "   Titre: CSI 4506 - DEVOIR 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMUnCICdyBbs"
      },
      "source": [
        "**Datasets dérivés**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-so3pwbPKTDX"
      },
      "source": [
        "Ce notebook est un point de départ pour le devoir 4. Dans ce devoir, vous effectuerez une étude empirique de classification. Ce notebook vous aidera à créer des ensembles de données dérivés dans la section 2 du devoir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bhFvS3q7Lu0R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (8.2.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.26.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: C:\\Users\\Karim\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.1.3)\n",
            "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.26.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: C:\\Users\\Karim\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --------------------------------------- 0.1/12.8 MB 660.6 kB/s eta 0:00:20\n",
            "      --------------------------------------- 0.3/12.8 MB 2.1 MB/s eta 0:00:06\n",
            "     - -------------------------------------- 0.5/12.8 MB 2.7 MB/s eta 0:00:05\n",
            "     -- ------------------------------------- 0.7/12.8 MB 3.3 MB/s eta 0:00:04\n",
            "     --- ------------------------------------ 1.0/12.8 MB 3.6 MB/s eta 0:00:04\n",
            "     --- ------------------------------------ 1.2/12.8 MB 3.9 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 1.5/12.8 MB 4.2 MB/s eta 0:00:03\n",
            "     ----- ---------------------------------- 1.8/12.8 MB 4.4 MB/s eta 0:00:03\n",
            "     ------ --------------------------------- 2.1/12.8 MB 4.7 MB/s eta 0:00:03\n",
            "     ------- -------------------------------- 2.5/12.8 MB 5.0 MB/s eta 0:00:03\n",
            "     -------- ------------------------------- 2.9/12.8 MB 5.2 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 3.1/12.8 MB 5.3 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 3.5/12.8 MB 5.5 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 3.9/12.8 MB 5.6 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.2/12.8 MB 5.8 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 4.6/12.8 MB 6.0 MB/s eta 0:00:02\n",
            "     --------------- ------------------------ 5.0/12.8 MB 6.1 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 6.3 MB/s eta 0:00:02\n",
            "     ------------------ --------------------- 5.9/12.8 MB 6.3 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 6.3/12.8 MB 6.5 MB/s eta 0:00:01\n",
            "     -------------------- ------------------- 6.6/12.8 MB 6.5 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 7.1/12.8 MB 6.7 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 7.3/12.8 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 7.7/12.8 MB 6.7 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 8.1/12.8 MB 6.7 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 8.2/12.8 MB 6.7 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 8.4/12.8 MB 6.5 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.8/12.8 MB 6.6 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.3/12.8 MB 6.7 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.5/12.8 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 9.9/12.8 MB 6.7 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 10.2/12.8 MB 6.7 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 10.6/12.8 MB 7.2 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 11.0/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 11.4/12.8 MB 7.5 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.6/12.8 MB 7.5 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.0/12.8 MB 7.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.6/12.8 MB 7.7 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.8/12.8 MB 7.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 7.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\karim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: C:\\Users\\Karim\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#let's start by installing spaCy, pandas and the en language\n",
        "!pip install spacy\n",
        "!pip install --upgrade pandas\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "aCWgl6PLKTDY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX9WQWGSwU2D"
      },
      "source": [
        "Vous avez reçu une liste de datasets dans la description du devoir. Choisissez l'un des datasets, fournissez le lien ci-dessous et lisez ce dataset à l'aide de pandas. Vous devez fournir un lien vers votre propre répertoire Github même si vous utilisez une version réduite d'un ensemble de données du répertoire de votre TA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSyg0jjC1jJa"
      },
      "source": [
        "Vous devrez ajouter une description de l'ensemble de données et votre justification des choix effectués pour obtenir les ensembles de données dérivés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "1Xx4qMCLKTDb"
      },
      "outputs": [],
      "source": [
        "#Load the dataset you chose.\n",
        "# Make sure the Notebook can load your dataset, just like previous assignments.\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_file_cnnnews.csv'\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_drugsComTest_raw_fiveclasses.csv'\n",
        "url = 'https://raw.githubusercontent.com/KarimBenhallam/csi4506_asg4/main/reduced_file_cnnnews.csv'\n",
        "\n",
        "#provide the link to the raw version of dataset. You *need* to provide a link to *your own* github repository. DO NOT use the link that is provided as an example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "wg24OUV81Xgm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://raw.githubusercontent.com/KarimBenhallam/csi4506_asg4/main/reduced_file_cnnnews.csv\n"
          ]
        }
      ],
      "source": [
        "print(url)\n",
        "data = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "AQ5nSY1HKTDd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Body</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Candy factory didn't evacuate concerned worker...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>accident investigations, accidents, accidents,...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/pennsylv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Baltimore police ask for public's help identif...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>baltimore, brand safety-nsf crime, brand safet...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/morgan-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>An arrest warrant has been issued for a suspec...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>arrest warrants, arrests, brand safety-nsf cri...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/josh-kru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>115 improperly stored human remains found in C...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>brand safety-nsf death, brand safety-nsf sensi...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/colorado...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Bronx day care provider and 2 others indicted ...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>brand safety-nsf crime, brand safety-nsf death...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/05/us/bronx-da...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                              Title  \\\n",
              "0   1  Candy factory didn't evacuate concerned worker...   \n",
              "1   2  Baltimore police ask for public's help identif...   \n",
              "2   3  An arrest warrant has been issued for a suspec...   \n",
              "3   4  115 improperly stored human remains found in C...   \n",
              "4   5  Bronx day care provider and 2 others indicted ...   \n",
              "\n",
              "                                         Description  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                            Keywords Theme  \\\n",
              "0  accident investigations, accidents, accidents,...    us   \n",
              "1  baltimore, brand safety-nsf crime, brand safet...    us   \n",
              "2  arrest warrants, arrests, brand safety-nsf cri...    us   \n",
              "3  brand safety-nsf death, brand safety-nsf sensi...    us   \n",
              "4  brand safety-nsf crime, brand safety-nsf death...    us   \n",
              "\n",
              "                                                Link  \n",
              "0  https://edition.cnn.com/2023/10/06/us/pennsylv...  \n",
              "1  https://edition.cnn.com/2023/10/06/us/morgan-s...  \n",
              "2  https://edition.cnn.com/2023/10/06/us/josh-kru...  \n",
              "3  https://edition.cnn.com/2023/10/06/us/colorado...  \n",
              "4  https://edition.cnn.com/2023/10/05/us/bronx-da...  "
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pycllPKTDd"
      },
      "source": [
        "Ici vous créez votre pipeline TAL. load() téléchargera le bon modèle pour l'analyse (English)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "wQtSi8XuKTDe"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqFArDyKTDf"
      },
      "source": [
        "L'application du pipeline à chaque phrase crée un document dans lequel chaque mot est un objet Token.\n",
        "\n",
        "Doc: https://spacy.io/api/doc\n",
        "\n",
        "Token: https://spacy.io/api/token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['ID', 'Title', 'Description', 'Body', 'Keywords', 'Theme', 'Link'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "WcaeMUL2KTDg"
      },
      "outputs": [],
      "source": [
        "#Apply nlp pipeline to the column that has your sentences (the text that will serve as input features).\n",
        "data['tokenized'] = data[\"Body\"].apply(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "7i6ai1I8KTDg"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Body</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Link</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Candy factory didn't evacuate concerned worker...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>accident investigations, accidents, accidents,...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/pennsylv...</td>\n",
              "      <td>(An, eastern, Pennsylvania, candy, factory, di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Baltimore police ask for public's help identif...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>baltimore, brand safety-nsf crime, brand safet...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/morgan-s...</td>\n",
              "      <td>(Two, shooters, were, involved, in, an, attack...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>An arrest warrant has been issued for a suspec...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>arrest warrants, arrests, brand safety-nsf cri...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/josh-kru...</td>\n",
              "      <td>(Authorities, in, Pennsylvania, say, they, hav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>115 improperly stored human remains found in C...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>brand safety-nsf death, brand safety-nsf sensi...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/colorado...</td>\n",
              "      <td>(An, investigation, into, more, than, 115, bod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Bronx day care provider and 2 others indicted ...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>brand safety-nsf crime, brand safety-nsf death...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/05/us/bronx-da...</td>\n",
              "      <td>(A, Bronx, day, care, provider, ,, her, husban...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                              Title  \\\n",
              "0   1  Candy factory didn't evacuate concerned worker...   \n",
              "1   2  Baltimore police ask for public's help identif...   \n",
              "2   3  An arrest warrant has been issued for a suspec...   \n",
              "3   4  115 improperly stored human remains found in C...   \n",
              "4   5  Bronx day care provider and 2 others indicted ...   \n",
              "\n",
              "                                         Description  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                            Keywords Theme  \\\n",
              "0  accident investigations, accidents, accidents,...    us   \n",
              "1  baltimore, brand safety-nsf crime, brand safet...    us   \n",
              "2  arrest warrants, arrests, brand safety-nsf cri...    us   \n",
              "3  brand safety-nsf death, brand safety-nsf sensi...    us   \n",
              "4  brand safety-nsf crime, brand safety-nsf death...    us   \n",
              "\n",
              "                                                Link  \\\n",
              "0  https://edition.cnn.com/2023/10/06/us/pennsylv...   \n",
              "1  https://edition.cnn.com/2023/10/06/us/morgan-s...   \n",
              "2  https://edition.cnn.com/2023/10/06/us/josh-kru...   \n",
              "3  https://edition.cnn.com/2023/10/06/us/colorado...   \n",
              "4  https://edition.cnn.com/2023/10/05/us/bronx-da...   \n",
              "\n",
              "                                           tokenized  \n",
              "0  (An, eastern, Pennsylvania, candy, factory, di...  \n",
              "1  (Two, shooters, were, involved, in, an, attack...  \n",
              "2  (Authorities, in, Pennsylvania, say, they, hav...  \n",
              "3  (An, investigation, into, more, than, 115, bod...  \n",
              "4  (A, Bronx, day, care, provider, ,, her, husban...  "
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHRfZ2uEKTDh"
      },
      "source": [
        "Un token a plusieurs attributs, tel part-of-speech (pos_), lemma (lemma_), etc. Regardez la documentation pour voir l'ensemble des attributs.\n",
        "\n",
        "La fonction suivante est un exemple de la façon dont vous pouvez récupérer les parties du discours (POS)à partir d'une phrase. Nous renvoyons la lemmatisation car nous voulons uniquement le mot à l'infinitif."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qw0a_2ySyUo2"
      },
      "outputs": [],
      "source": [
        "#create empty dataframes that will store your derived datasets\n",
        "\n",
        "derived_dataset1 = pd.DataFrame(columns = ['Class', 'pos'])\n",
        "derived_dataset2 = pd.DataFrame(columns = ['Class', 'pos-np'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Yeak1tAOKTDi"
      },
      "outputs": [],
      "source": [
        "def get_pos(sentence, pos_list): #wanted_pos refers to the desired pos tagging\n",
        "   return [token.lemma_ for token in sentence if token.pos_ in pos_list]\n",
        "\n",
        "def get_entities_and_verbs(sentence):\n",
        "    entities = [ent.text for ent in sentence.ents if ent.label_ in ['PERSON', 'ORG']]\n",
        "    verbs = [token.lemma_ for token in sentence if token.pos_ == 'VERB']\n",
        "    return entities + verbs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "147NRzwKKTDj"
      },
      "outputs": [],
      "source": [
        "# Derived-Dataset-1 : Noms et Adjectifs liés aux Personnes et Lieux\n",
        "derived_dataset1['pos'] = data['tokenized'].apply(lambda sent: get_pos(sent, ['PROPN', 'NOUN', 'ADJ']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "G_bUg_fVKTDk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[eastern, Pennsylvania, candy, factory, employ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[shooter, attack, Morgan, State, University, B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[authority, Pennsylvania, arrest, warrant, man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[investigation, more, body, Colorado, funeral,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Bronx, day, care, provider, husband, cousin, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Class                                                pos\n",
              "0   NaN  [eastern, Pennsylvania, candy, factory, employ...\n",
              "1   NaN  [shooter, attack, Morgan, State, University, B...\n",
              "2   NaN  [authority, Pennsylvania, arrest, warrant, man...\n",
              "3   NaN  [investigation, more, body, Colorado, funeral,...\n",
              "4   NaN  [Bronx, day, care, provider, husband, cousin, ..."
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "derived_dataset1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "AuGv-NnfKTDj"
      },
      "outputs": [],
      "source": [
        "# Derived-Dataset-2 : Personnes et Organisations + Verbes d'Action\n",
        "derived_dataset2['pos-np'] = data['tokenized'].apply(get_entities_and_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "NR7AdW0MfXO6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        }
      ],
      "source": [
        "#For Derived Dataset 2, you also need to include Named Entities\n",
        "#Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
        "#on the dataset of your choice.\n",
        "#You can choose the types of entities (dates, organization, people) that you want,\n",
        "#and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
        "#in a previous cell.\n",
        "\n",
        "sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pX4RgKhKTDk"
      },
      "source": [
        "Maintenant que vous disposez de vos ensembles de données dérivés, vous pouvez effectuer votre tâche de classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GhniwHtzfQt"
      },
      "source": [
        "**Classification avec MLP et Régression logistique**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilisez la méthode CountVectorizer ou TfidfVectorizer de scikit-learn pour transformer le texte en un sac de mots (bag-of-words). Vous pouvez également supprimer les mots vides pour réduire le nombre de mots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Karim\\Desktop\\classes\\Fall 2023\\CSI 4506\\devoir 4\\csi4506_asg4\\CSI4506_A4_Modele.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Karim/Desktop/classes/Fall%202023/CSI%204506/devoir%204/csi4506_asg4/CSI4506_A4_Modele.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m CountVectorizer, TfidfVectorizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Karim/Desktop/classes/Fall%202023/CSI%204506/devoir%204/csi4506_asg4/CSI4506_A4_Modele.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Karim/Desktop/classes/Fall%202023/CSI%204506/devoir%204/csi4506_asg4/CSI4506_A4_Modele.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Utiliser CountVectorizer ou TfidfVectorizer\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Utiliser CountVectorizer ou TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Utilisez TfidfVectorizer avec 5000 features max\n",
        "X = vectorizer.fit_transform(data['text'])  # 'text' est la colonne contenant le texte\n",
        "\n",
        "# Séparer les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Définition des modèles:\n",
        "\n",
        "Regression logistique:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Créer le modèle de régression logistique\n",
        "logistic_model = LogisticRegression(solver='lbfgs', max_iter=100)\n",
        "\n",
        "# Entraîner le modèle\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Évaluer le modèle\n",
        "accuracy_logistic = logistic_model.score(X_test, y_test)\n",
        "print(f\"Accuracy of Logistic Regression Model: {accuracy_logistic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Créer le modèle perceptron multicouches\n",
        "mlp_model = MLPClassifier(max_iter=300)\n",
        "\n",
        "# Entraîner le modèle\n",
        "mlp_model.fit(X_train, y_train)\n",
        "\n",
        "# Évaluer le modèle\n",
        "accuracy_mlp = mlp_model.score(X_test, y_test)\n",
        "print(f\"Accuracy of MLP Model: {accuracy_mlp}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrainement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "# Définir les modèles\n",
        "logistic_model = LogisticRegression(solver='lbfgs', max_iter=100)\n",
        "mlp_model = MLPClassifier(max_iter=300)\n",
        "\n",
        "# Définir les datasets\n",
        "datasets = [derived_dataset1, derived_dataset2]\n",
        "\n",
        "# Définir les métriques à calculer\n",
        "metrics = ['precision_micro', 'precision_macro', 'recall_micro', 'recall_macro', 'f1_micro', 'f1_macro']\n",
        "\n",
        "for dataset in datasets:\n",
        "    # Diviser le dataset en attributs (X) et labels (y)\n",
        "    X = dataset.drop('label', axis=1) \n",
        "    y = dataset['label']\n",
        "\n",
        "    # Cross-validation\n",
        "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "    # Logistic Regression\n",
        "    logistic_scores = cross_val_score(logistic_model, X, y, cv=skf, scoring='accuracy')\n",
        "    print(f\"Logistic Regression Cross-Validation Accuracy: {np.mean(logistic_scores)}\")\n",
        "\n",
        "    # MLP\n",
        "    mlp_scores = cross_val_score(mlp_model, X, y, cv=skf, scoring='accuracy')\n",
        "    print(f\"MLP Cross-Validation Accuracy: {np.mean(mlp_scores)}\")\n",
        "\n",
        "    # Évaluer avec des mesures de précision, rappel, et f1\n",
        "    for metric in metrics:\n",
        "        precision = precision_score(y, mlp_model.predict(X), average=metric.split('_')[1])\n",
        "        recall = recall_score(y, mlp_model.predict(X), average=metric.split('_')[1])\n",
        "        f1 = f1_score(y, mlp_model.predict(X), average=metric.split('_')[1])\n",
        "        print(f\"MLP {metric}: Precision={precision}, Recall={recall}, F1={f1}\")\n",
        "\n",
        "\n",
        "# Experiment 1: Changer la taille de la couche cachée\n",
        "mlp_model_exp1 = MLPClassifier(hidden_layer_sizes=(50,), max_iter=300)\n",
        "mlp_scores_exp1 = cross_val_score(mlp_model_exp1, X, y, cv=skf, scoring='accuracy')\n",
        "print(f\"MLP Experiment 1 Cross-Validation Accuracy: {np.mean(mlp_scores_exp1)}\")\n",
        "\n",
        "# Experiment 2: Changer la fonction d'activation\n",
        "mlp_model_exp2 = MLPClassifier(activation='tanh', max_iter=300)\n",
        "mlp_scores_exp2 = cross_val_score(mlp_model_exp2, X, y, cv=skf, scoring='accuracy')\n",
        "print(f\"MLP Experiment 2 Cross-Validation Accuracy: {np.mean(mlp_scores_exp2)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
